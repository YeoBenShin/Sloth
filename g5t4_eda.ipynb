{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58923129-67a7-496b-a962-85e35e0cc433",
   "metadata": {},
   "source": [
    "# Part 1: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574fbe6-7383-4f67-8869-dd7c0b17168b",
   "metadata": {},
   "source": [
    "**1. Overview of dataset [15%]**\n",
    "a. Summarize the background of the dataset.\n",
    "b. State the size of the dataset.\n",
    "c. For each variable, describe what it represents and its data type (numerical or categorical).\n",
    "\n",
    "**2. Data pre-processing [35%]**\n",
    "a. For each variable, determine the percentage of missing data. For any column with missing data, describe how you resolve the issue. Clearly state any assumption you made.\n",
    "b. For each variable, identify outliers (if any) and describe how you resolve the issue. Clearly state any assumption you made.\n",
    "c. For categorical variables, perform the necessary encoding.\n",
    "\n",
    "**3. Exploratory analysis and visualization [50%]**\n",
    "a. For each variable, provide relevant summary statistics.\n",
    "b. For each variable, provide an appropriate visualisation depicting the distribution of its values, and summarize any key observation(s) you made.\n",
    "c. Perform bi-variate analyses on the variables. You do not need to analyse every pair; only focus on the pairs you believe are worth investigating and explain your choices. For each pair, describe the relationship between the two variables. Use appropriate statistical methods and/or visualization.\n",
    "\n",
    "If applicable, corresponding codes that are reproducible (i.e., they produce output consistent with your answers), and well documented (in the form of comments and markdown cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99466495-6926-4267-b915-0ab7bf38af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1a344-e8fb-43fe-a95c-106a6e15d099",
   "metadata": {},
   "source": [
    "## Reading of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc62bf-a157-4aaf-a45d-fd6bec65f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"employee.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136730d-9e6d-4001-bf2b-12242deb5f92",
   "metadata": {},
   "source": [
    "## 1. Overview of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1434aa-9f5f-4466-9294-8cc971ef7b06",
   "metadata": {},
   "source": [
    "### a. Summarize the background of the dataset.\n",
    "The dataset contains HR data related to a sales team. Specifically the dataset captures employeesâ€™ information at the beginning of each month, including if the employee quit in that given month, total sales acquired so far in his/her career and his/her latest quarterly rating. The dataset contains 2381 rows and 13 columns.\n",
    "\n",
    "| Column Name | Description | Type | Details |\n",
    "| ----------- | ----------- | ---- | ------- |\n",
    "| Date | Record capture date | Categorical | Date in format DD/MM/YYYY |\n",
    "| Emp_ID | Employee ID | Categorical |\n",
    "| Age | Age | Numerical |\n",
    "| Gender | Gender | Categorical | \"Male\", \"Female\" |\n",
    "| City | City where employee works | Categorical | \"C1\", \"C2\", ..., \"C29\" |\n",
    "| Education | Highest education level of employee | Categorical | \"College\", \"Bachelor\", \"Master\" |\n",
    "| Salary | Last drawn salary | Numerical |\n",
    "| Join_Date | Date of joining the sales team | Categorical | Date in format DD/MM/YYYY |\n",
    "| Last_Work_Date | Last working date with the sales team | Categorical | Date in format DD/MM/YYYY. If employee does not quit in the given month where record is captured, the field would be empty (NaN). |\n",
    "| Join_Designation | Designation level when first joined | Categorical | 1, 2, 3, 4, 5. |\n",
    "| Designation | Designation level at the time record is captured | Categorical | 1, 2, 3, 4, 5 |\n",
    "| Total_Sales_Acquired | Total sales generated by employee since joining the team | Numerical |\n",
    "| Quarterly_Rating | Latest quarterly performance rating | Categorical | 1, 2, 3, 4 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07980215-16a1-485f-9fdc-3c32a351ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows and columns (size) of dataframe\n",
    "n_rows, n_cols = df.shape\n",
    "\n",
    "print(f\"Number of rows: {n_rows}\")\n",
    "print(f\"Number of columns: {n_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef326da9-ecf6-4348-96c7-8fe50013f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datatype of attributes/features\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd77bb7-8b95-4c69-94ad-443e40b35a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysing unique values of specific columns\n",
    "genders = df[\"Gender\"].unique()\n",
    "cities = df[\"City\"].unique()\n",
    "education_levels = df[\"Education\"].unique()\n",
    "join_designation_levels = df[\"Join_Designation\"].unique()\n",
    "designation_levels = df[\"Designation\"].unique()\n",
    "quarterly_ratings = df[\"Quarterly_Rating\"].unique()\n",
    "\n",
    "print(f\"Possible values of 'Gender' attribute: {genders}\")\n",
    "print(f\"Possible values of 'City' attribute: {cities}\")\n",
    "print(f\"Possible values of 'Education' attribute: {education_levels}\")\n",
    "print(f\"Possible values of 'Join_Designation' attribute: {join_designation_levels}\")\n",
    "print(f\"Possible values of 'Designation' attribute: {designation_levels}\")\n",
    "print(f\"Possible values of 'Quarterly_Rating' attribute: {quarterly_ratings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49ae15-f9b9-44cd-bd27-9cf500e39273",
   "metadata": {},
   "source": [
    "## 2. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e5bcb-88d3-4c40-8d71-8c7df0280162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of available data\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4109b-087a-459d-9183-d3d4c7e2c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of data missing per attribute\n",
    "print(\"Percentage of data missing\")\n",
    "\n",
    "col = df.columns\n",
    "i = 0\n",
    "missing_data_col = []\n",
    "\n",
    "for valid_data in df.count():\n",
    "    print(col[i], end=\" = \")\n",
    "    per_missing = (1 - (valid_data/n_rows))*100\n",
    "    print(f\"{per_missing : .2f}%\")\n",
    "\n",
    "    if per_missing > 0:\n",
    "        missing_data_col.append(col[i])\n",
    "        \n",
    "    i += 1\n",
    "print()\n",
    "\n",
    "# print all column names of columns with missing data\n",
    "print(\"Columns with missing data: \" + str(missing_data_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74690b22-1c50-44c5-a36a-cdd9fdb63115",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "| Column Name | Percentage Missing | Resolution Technique | Assumptions |\n",
    "| ----------- | ------------------ | -------------------- | ----------- |\n",
    "| Last_Work_Date | 32.13% | Impute NaN values as the date in the \"Date\" attribute and Impute a new attribute \"Current_Staff\" with those that have NaN values initialised as \"Yes\" and those that have Last_Work_Date as \"No\". | Fields with NaN values suggest that the staff is still with the company. |\n",
    "| Join_Date | 4.96% | Impute NaN values as the average tenure of their salary group from their Last_Work_Date | There is a correlation between salary and the tenure length of a worker in the company. |\n",
    "| Join_Designation | 4.41% | Impute the NaN values as 1. | All workers begin at the lowest Designation level when they joined the company. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286da615-eb35-46e6-adf0-34fe767abf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolving Last_Work_Date\n",
    "print(f\"Last_Work_Date has been resolved: {df[\"Last_Work_Date\"].count() == n_rows}\")\n",
    "df[\"Current_Staff\"] = \"No\"\n",
    "df.loc[df[\"Last_Work_Date\"].isna(), \"Current_Staff\"] = \"Yes\"\n",
    "df.loc[df[\"Last_Work_Date\"].isna(), \"Last_Work_Date\"] = df[\"Date\"]\n",
    "print(f\"Last_Work_Date has been resolved: {df[\"Last_Work_Date\"].count() == n_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(f\"Join_Date has been resolved: {df[\"Join_Date\"].count() == n_rows}\")\n",
    "\n",
    "# Convert date columns to datetime\n",
    "df['Join_Date'] = pd.to_datetime(df['Join_Date'], format='%d/%m/%Y', errors='coerce')\n",
    "df['Last_Work_Date'] = pd.to_datetime(df['Last_Work_Date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Filter employees with missing Join_Date\n",
    "missing_join_df = df[df['Join_Date'].isna()].copy()\n",
    "\n",
    "# Filter employees with both Join_Date and Last_Work_Date\n",
    "valid_join_df = df[df['Join_Date'].notna() & df['Last_Work_Date'].notna()].copy()\n",
    "\n",
    "# Calculate tenure in days for employees with valid join and last work dates\n",
    "valid_join_df.loc[:, 'Tenure'] = (valid_join_df['Last_Work_Date'] - valid_join_df['Join_Date']).dt.days\n",
    "\n",
    "# Group by salary ranges (e.g., every 10,000)\n",
    "valid_join_df.loc[:, 'Salary_Range'] = pd.cut(valid_join_df['Salary'], bins=range(0, 150000, 10000))\n",
    "\n",
    "# Calculate average tenure for each salary range\n",
    "avg_tenure_by_salary = valid_join_df.groupby('Salary_Range', observed=True)['Tenure'].mean()\n",
    "\n",
    "# Function to estimate join date based on salary range\n",
    "def estimate_join_date(row):\n",
    "    salary_range = pd.cut([row['Salary']], bins=range(0, 150000, 10000))[0]\n",
    "    if salary_range in avg_tenure_by_salary:\n",
    "        avg_tenure = avg_tenure_by_salary[salary_range]\n",
    "        if pd.notna(row['Last_Work_Date']):\n",
    "            estimated_join_date = row['Last_Work_Date'] - pd.Timedelta(days=avg_tenure)\n",
    "            return estimated_join_date\n",
    "    return row['Last_Work_Date'] - pd.DateOffset(years=1)  # Default fallback\n",
    "\n",
    "# Update only the rows with missing join dates\n",
    "df.loc[missing_join_df.index, 'Join_Date'] = missing_join_df.apply(estimate_join_date, axis=1)\n",
    "\n",
    "# Convert the estimated join date back to the original format\n",
    "df['Join_Date'] = df['Join_Date'].dt.strftime('%d/%m/%Y')\n",
    "df['Last_Work_Date'] = df['Last_Work_Date'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Print the updated DataFrame\n",
    "# print(df[['Emp_ID', 'Salary', 'Date', 'Join_Date']])\n",
    "# print(df['Join_Date'].count() / df['Emp_ID'].count() * 100)\n",
    "\n",
    "# Verify all Join_Date values are filled\n",
    "print(f\"Join_Date has been resolved: {df['Join_Date'].count() == n_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec09ca-bf6f-4d63-a3df-aacf2f6ef6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolving Join_Designation\n",
    "print(f\"Join_Designation has been resolved: {df[\"Join_Designation\"].count() == n_rows}\")\n",
    "df[\"Join_Designation\"] = df[\"Join_Designation\"].replace(np.nan, 1)\n",
    "\n",
    "print(f\"Join_Designation has been resolved: {df[\"Join_Designation\"].count() == n_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8fe852-d75b-400e-bc6c-4858c02ee626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all NaN values have been handled\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b09c7f",
   "metadata": {},
   "source": [
    "### Invalid Data Identification\n",
    "\n",
    "We will now be checking if there are any outliers in the dataset provided. \n",
    "\n",
    "We will do so by checking the min and max value of each variables and see if there are any values that is outside of the norm of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the min of each variable\n",
    "print(\"Min value of each variable\\n\")\n",
    "col = df.columns\n",
    "for i in range(len(df.columns)):\n",
    "    print(col[i], end=\" = \")\n",
    "    min_value = df[col[i]].min()\n",
    "    print(str(min_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd5eed3",
   "metadata": {},
   "source": [
    "As seen from the results, the minimum value of the Total_Sales_Acquired attribute is negative. This is an outlier as we assumed that employees can only sales or no sales at all. There should not be employees that make negative sales.\n",
    "\n",
    "Hence, we will be removing all the employees that are making negative sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952911b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of employees that make negative sales\n",
    "negative_sales = df.loc[df[\"Total_Sales_Acquired\"] < 0, \"Total_Sales_Acquired\"].count()\n",
    "print(\"Employees with negative sales: \" + str(negative_sales))\n",
    "print(\"Percentage of employees with negative sales out of all employees: \" + str((negative_sales / df['Total_Sales_Acquired'].count())*100) + \"%\")\n",
    "\n",
    "# removing the aforementioned employees from the data\n",
    "df = df.drop(df[df['Total_Sales_Acquired'] < 0].index)\n",
    "\n",
    "# check again\n",
    "print(\"Updated employees with negative sales: \" + str(df.loc[df[\"Total_Sales_Acquired\"] < 0, \"Total_Sales_Acquired\"].count()))\n",
    "print(\"Min. Total Sales Acquired: \" + str(df[\"Total_Sales_Acquired\"].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0da65",
   "metadata": {},
   "source": [
    "We will now be checking the max value of each variables and see if there are any values that is outside of the norm of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa259a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the max of each variable\n",
    "print(\"Max value of each variable\\n\")\n",
    "col = df.columns\n",
    "for i in range(len(df.columns)):\n",
    "    print(col[i], end=\" = \")\n",
    "    min_value = df[col[i]].max()\n",
    "    print(str(min_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98d4a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of available data\n",
    "print(df.count())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d52f1-34f1-4674-a7cd-673970823f99",
   "metadata": {},
   "source": [
    "### How we resolved the invalid data\n",
    "\n",
    "| Column Name | Problem | Percentage Invalid | Resolution Technique | Assumptions |\n",
    "| ----------- | ------- | ------------------ | -------------------- | ----------- |\n",
    "| Total_Sales_Acquired| Some of the records have Total_Sales_Acquired that are negative | 0.41999160016799664% | Since there are only 10 or 0.420% of the total records that show invalid data, we have decided to remove those records. | We assume that negative Total_Sales_Acquired is not possible, hence suggesting that the data is invalid. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adacd302-41ca-4fcb-947c-8dec509188a1",
   "metadata": {},
   "source": [
    "### Outlier Identification\n",
    "\n",
    "We must now analyze the dataset provided, and identify the outliers from selected variables.\n",
    "\n",
    "The selection process lies solely on intuition. For example, we chose age for there will, of course, be a possibility that an employee would be outside of the hiring norms.\n",
    "\n",
    "All of these side, we chose three variables to analyze:\n",
    "1. Age\n",
    "2. Salary\n",
    "3. Sales\n",
    "\n",
    "For each variable, we identify the following:\n",
    "1. 25th Quartile\n",
    "2. 75th Quartile\n",
    "3. Interquartile range (IQR)\n",
    "\n",
    "And use the obtained information to identify the upper and lower boundaries. With these on hand, we can then identify the outliers by checking if the data goes beyond the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68a228-9b0e-45b5-90c7-a2dbd68d71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Age\n",
    "# quantiles\n",
    "AGEQ1 = df[\"Age\"].quantile(0.25)\n",
    "AGEQ3 = df[\"Age\"].quantile(0.75)\n",
    "AGEIQR = AGEQ3-AGEQ1\n",
    "\n",
    "# boundaries\n",
    "AGEUpper = AGEQ3+(1.5*AGEIQR)\n",
    "AGELower = AGEQ1-(1.5*AGEIQR)\n",
    "AGEOutlier = df[(df[\"Age\"]<AGELower)|(df[\"Age\"]>AGEUpper)]\n",
    "print(\"AGE 25th Percentile:\", AGEQ1)\n",
    "print(\"Youngest employee:\", df[\"Age\"].min())\n",
    "print(\"AGE 75th Percentile:\", AGEQ3)\n",
    "print(\"Oldest employee:\", df[\"Age\"].max())\n",
    "print(\"AGE IQR:\", AGEIQR)\n",
    "print(\"Total amount of AGE Outliers:\", len(AGEOutlier))\n",
    "\n",
    "# this one prints everything idk how to pinpoint the employee ids only\n",
    "# print(\"AGE Outliers\", AGEOutlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83751160-d4fd-4a11-b680-7b0884312010",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Salary\n",
    "# quantiles\n",
    "SALQ1 = df[\"Salary\"].quantile(0.25)\n",
    "SALQ3 = df[\"Salary\"].quantile(0.75)\n",
    "SALIQR = SALQ3-SALQ1\n",
    "\n",
    "# boundaries\n",
    "SALUpper = SALQ3+(1.5*SALIQR)\n",
    "SALLower = SALQ1-(1.5*SALIQR)\n",
    "SALOutlier = df[(df[\"Salary\"]<SALLower)|(df[\"Salary\"]>SALUpper)]\n",
    "print(\"SALARY 25th Percentile: $\", SALQ1)\n",
    "print(\"Lowest salary: $\", df[\"Salary\"].min())\n",
    "print(\"SALARY 75th Percentile: $\", SALQ3)\n",
    "print(\"Highest salary: $\", df[\"Salary\"].max())\n",
    "print(\"Salary IQR: $\", SALIQR)\n",
    "print(\"Total amount of Salary Outliers:\", len(SALOutlier))\n",
    "\n",
    "# this one prints everything idk how to pinpoint the employee ids only\n",
    "# print(\"Salary Outliers\", SALOutlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54373fc-7196-4c45-9b5b-5129a50d1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Salary\n",
    "# quantiles\n",
    "\n",
    "# need to filter again for zeroes if thats a concern\n",
    "# i understand that negatives got removed\n",
    "# this one includes zeroes\n",
    "\n",
    "SALESQ1 = df[\"Total_Sales_Acquired\"].quantile(0.25)\n",
    "SALESQ3 = df[\"Total_Sales_Acquired\"].quantile(0.75)\n",
    "SALESIQR = SALESQ3-SALESQ1\n",
    "\n",
    "# boundaries\n",
    "SALESUpper = SALESQ3+(1.5*SALESIQR)\n",
    "SALESLower = SALESQ1-(1.5*SALESIQR)\n",
    "SALESOutlier = df[(df[\"Total_Sales_Acquired\"]<SALESLower)|(df[\"Total_Sales_Acquired\"]>SALESUpper)]\n",
    "print(\"SALES 25th Percentile: $\", SALESQ1)\n",
    "print(\"Lowest sales: $\", df[\"Total_Sales_Acquired\"].min())\n",
    "print(\"SALES 75th Percentile: $\", SALESQ3)\n",
    "print(\"Highest sales: $\", df[\"Total_Sales_Acquired\"].max())\n",
    "print(\"Sales IQR: $\", SALESIQR)\n",
    "print(\"Total amount of Sales Outliers:\", len(SALESOutlier))\n",
    "\n",
    "# this one prints everything idk how to pinpoint the employee ids only\n",
    "# print(\"Salary Outliers\", SALOutlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9351de6-abb4-4374-99cd-b15a722e6ad3",
   "metadata": {},
   "source": [
    "## 3. EDA and Visualisation\n",
    "\n",
    "1. Univariate\n",
    "- histogram for Total_Sales_Acquired\n",
    "- histogram for Total_Sales_Acquired without employees with 0 sales\n",
    "- donut chart for Designation_Level distribution\n",
    "- pie chart for Quarterly_Rating distribution\n",
    "- histogram for Salary distribution\n",
    "- histogram for Age distribution\n",
    "- pie chart for Gender distribution\n",
    "- pie chart for Education level distribution \n",
    "- bar chart for City distribution \n",
    "\n",
    "\n",
    "2. Bivariate\n",
    "- grouped boxplot for Salary against Education level\n",
    "- grouped boxplot for Salary against designation\n",
    "- grouped boxplot for Salary against Gender\n",
    "- scatter plot for Salary against Age\n",
    "- grouped boxplot for Total_Sales_Acquired and Gender\n",
    "- scatter plot for Total_Sales_Acquired against Age\n",
    "- scatter plot for Total_Sales_Acquired against Salary\n",
    "- histogram for Total_Sales_Acquired by Designation_Level \n",
    "- histogram for Total_Sales_Acquired by Quarterly_Rating "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8579c0",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdfc3ff-7582-4354-b7b4-31295d518ac4",
   "metadata": {},
   "source": [
    "### Total_Sales_Acquired\n",
    "With a large standard deviation, it goes to show that the spread of the Total_Sales_Acquired across all employees in the company is rather large. \n",
    "\n",
    "The histogram plot also suggests that employees either produce very little to no sales or a lot of sales. Perhaps this suggests that the company has a significant proportion of employees that are not involved in sales. For more meaningful analysis, it could perhaps be more useful to categorise the type of employees.\n",
    "\n",
    "Alternatively, this might also indicates that some employees may be facing challenges or are underperforming. However, there is also a group of employees that achieved high sales reaching 10-18 which gives a clear difference between low and high performers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81faf4ea-9e9f-42fd-8832-9e33fe340dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "print(f\"Mean total sales acquired: {df[\"Total_Sales_Acquired\"].mean() : .2f}\")\n",
    "\n",
    "# standard deviation\n",
    "print(f\"s.d. total sales acquired: {df[\"Total_Sales_Acquired\"].std() : .2f}\")\n",
    "\n",
    "# min\n",
    "print(f\"Min total sales acquired: {df[\"Total_Sales_Acquired\"].min() : .2f}\")\n",
    "\n",
    "# median \n",
    "print(f\"Median total sales acquired: {df[\"Total_Sales_Acquired\"].median() : .2f}\")\n",
    "\n",
    "# max\n",
    "print(f\"Max total sales acquired: {df[\"Total_Sales_Acquired\"].max() : .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be63fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Total Sales Acquired\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Total Sales Acquired\", fontsize=15)\n",
    "plt.xlabel(\"Sales\", fontsize=12)\n",
    "plt.ylabel(\"Num of Employees\", fontsize=12)\n",
    "log_TotalSales = np.log1p(df.Total_Sales_Acquired)\n",
    "log_TotalSales.hist(bins=10) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542c897",
   "metadata": {},
   "source": [
    "### Total_Sales_Acquired without Employees with 0 sales\n",
    "\n",
    "With the revelation that there are many employees that have 0 sales, we decided to do a total sales acquired analysis only on employees that has sales so that the insights gathered are more meaningful.\n",
    "\n",
    "Excluding the 719 employees with 0 sales, the mean total sales is actually about 2 million more than previously analysed. This shows that the employees making sales are doing a much better job than previously analysed. Furthermore, with reference to the histogram, the total sales acquired does follow the a normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of employees with 0 sales\n",
    "print(\"Number of employees with 0 sales: \" + str(df.loc[df[\"Total_Sales_Acquired\"] == 0, \"Total_Sales_Acquired\"].count()))\n",
    "\n",
    "# mean\n",
    "print(f\"Mean total sales acquired: {df.loc[df[\"Total_Sales_Acquired\"] > 0, \"Total_Sales_Acquired\"].mean() : .2f}\")\n",
    "\n",
    "# standard deviation\n",
    "print(f\"s.d. total sales acquired: {df.loc[df[\"Total_Sales_Acquired\"] > 0, \"Total_Sales_Acquired\"].std() : .2f}\")\n",
    "\n",
    "# min\n",
    "print(f\"Min total sales acquired: {df.loc[df[\"Total_Sales_Acquired\"] > 0, \"Total_Sales_Acquired\"].min() : .2f}\")\n",
    "\n",
    "# median \n",
    "print(f\"Median total sales acquired: {df.loc[df[\"Total_Sales_Acquired\"] > 0, \"Total_Sales_Acquired\"].median() : .2f}\")\n",
    "\n",
    "# max is the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Total Sales Acquired\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Total Sales Acquired without Employees with 0 sales\", fontsize=15)\n",
    "plt.xlabel(\"Sales\", fontsize=12)\n",
    "plt.ylabel(\"Num of Employees\", fontsize=12)\n",
    "log_TotalSales = np.log1p(df.loc[df[\"Total_Sales_Acquired\"] > 0, \"Total_Sales_Acquired\"])\n",
    "log_TotalSales.hist(bins=10)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a213f1-f4fb-4630-9d34-ecee0080ebe7",
   "metadata": {},
   "source": [
    "### Designation_Level\n",
    "It suggests that the Designation level 5 could correspond to the Directors of the company and with each Designation level in descending order suggesting a more junior level staff of the company. This is representative of a typical corporate organisational structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "print(f\"Mean designation level: {df[\"Designation\"].mean() : .2f}\")\n",
    "\n",
    "# standard deviation\n",
    "print(f\"s.d. designation level: {df[\"Designation\"].std() : .2f}\")\n",
    "\n",
    "# min\n",
    "print(\"Min designation level: \" + str(df[\"Designation\"].min()))\n",
    "\n",
    "# median \n",
    "print(\"Median designation level: \" + str(df[\"Designation\"].median()))\n",
    "\n",
    "# max\n",
    "print(\"Max designation level: \" + str(df[\"Designation\"].max()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doughnut Chart showing the Percentage of Workers in Each Designation Level\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(df[\"Designation\"].value_counts().sort_index(), labels=df[\"Designation\"].unique(), autopct='%1.1f%%', wedgeprops={'edgecolor': 'white'}, pctdistance=.8)\n",
    "\n",
    "# Create a circle at the center to make it a doughnut\n",
    "centre_circle = plt.Circle((0, 0), 0.40, fc='white')\n",
    "plt.gca().add_artist(centre_circle)\n",
    "\n",
    "# Title & Show Plot\n",
    "plt.title(\"Percentage of Workers in Each Designation Level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463f979-eee7-4cbb-bc26-33bd475b4e1f",
   "metadata": {},
   "source": [
    "### Quarterly_Rating\n",
    "The pie chart shows a breakdown of the proportion of employees in for each Quarterly_Rating band. \n",
    "\n",
    "Intuitively, this suggests that band 4 which makes up the smallest proportion of employees at 4.5% are the top performers for the quarter. A vast majority of employees are awarded band 1 at 73.2% of the all employees in the company.\n",
    "\n",
    "This noticeable performance difference where a majority of employees received the lowest rating, might indicate widespread underperformance. The small number of employees with high ratings can also further suggest challenges in performance and lack of staff training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1270c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "print(f\"Mean quarterly rating: {df[\"Quarterly_Rating\"].mean() : .2f}\")\n",
    "\n",
    "# standard deviation\n",
    "print(f\"s.d. quarterly rating: {df[\"Quarterly_Rating\"].std() : .2f}\")\n",
    "\n",
    "# min\n",
    "print(\"Min quarterly rating: \" + str(df[\"Quarterly_Rating\"].min()))\n",
    "\n",
    "# median \n",
    "print(\"Median quarterly rating: \" + str(df[\"Quarterly_Rating\"].median()))\n",
    "\n",
    "# max\n",
    "print(\"Max quarterly rating: \" + str(df[\"Quarterly_Rating\"].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefed524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart showing the Percentage of Workers with Each Quarterly Rating\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(df[\"Quarterly_Rating\"].value_counts().sort_index(), labels=sorted(df[\"Quarterly_Rating\"].unique()), autopct='%1.1f%%', wedgeprops={'edgecolor': 'white'}, pctdistance=.6)\n",
    "\n",
    "# Title & Show Plot\n",
    "plt.title(\"Percentage of Workers with Each Quarterly Rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5edd326-8bff-4ab9-8e2f-151f0d88a08f",
   "metadata": {},
   "source": [
    "### Salary of Employees\n",
    "The mean salary of employees in the company is $59336.13. Based on the histogram, the distributions of salary of employees seem to follow a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975f52f-01b9-4de1-81ec-e8ae7e50c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean employee salary: {df['Salary'].mean() : .2f}\")\n",
    "\n",
    "print(f\"s.d. employee salary: {df['Salary'].std() : .2f}\")\n",
    "\n",
    "# min\n",
    "print(\"Min employee salary: \" + str(df[\"Salary\"].min()))\n",
    "\n",
    "# median \n",
    "print(\"Median employee salary: \" + str(df[\"Salary\"].median()))\n",
    "\n",
    "# max\n",
    "print(\"Max employee salary: \" + str(df[\"Salary\"].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87983b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Salary of Employees\", fontsize=15)\n",
    "plt.xlabel(\"Salary\", fontsize=12)\n",
    "plt.ylabel(\"Num of Employee\", fontsize=12)\n",
    "df[\"Salary\"].hist(bins=20)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60021d27-7a58-43b8-b95f-ef02bcebc028",
   "metadata": {},
   "source": [
    "### Age of Employees\n",
    "Based on the histogram, it seems to imply that majority of the employees in the company are around 30 years of age. \n",
    "\n",
    "The mean age is 33.7 which suggests that the distribution of the age of employees tend more towards larger than 30 years of age than it does to less than 30 years of age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309738a-5eac-4f81-a701-9f3b09cb892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean age of employee: {df['Age'].mean() : .0f}\")\n",
    "\n",
    "print(f\"s.d. age of employee: {df['Age'].std() : .0f}\")\n",
    "\n",
    "# min\n",
    "print(\"Min age of employee: \" + str(df[\"Age\"].min()))\n",
    "\n",
    "# median \n",
    "print(\"Median age of employee: \" + str(df[\"Age\"].median()))\n",
    "\n",
    "# max\n",
    "print(\"Max age of employee: \" + str(df[\"Age\"].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for Age\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Age of Employees\", fontsize=15)\n",
    "plt.xlabel(\"Age\", fontsize=12)\n",
    "plt.ylabel(\"Num of Employee\", fontsize=12)\n",
    "# log_Age = np.log1p(df.Age)\n",
    "df[\"Age\"].hist(bins=10)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366419f",
   "metadata": {},
   "source": [
    "### Gender Distribution\n",
    "The pie chart shows the gender distribution among employees, revealing that 59.0% of the workforce is male, while 41.0% is female.\n",
    "\n",
    "This illustrates a small gender gap with males taking the majority. The male-dominated environment may also be caused by industry trends, workplace policies or societal factors. Furthermore, it may also indicate a bias in hiring male employees because of the job requirements that are aligned more with male employees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart showing the Gender Distribution of the company\n",
    "x = df['Gender'].value_counts()\n",
    "y = x.index\n",
    "plt.figure(figsize=(6, 6)) #size\n",
    "plt.pie(x, labels=y, autopct='%1.1f%%') #string format\n",
    "plt.title(\"Gender Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7da45",
   "metadata": {},
   "source": [
    "### Education Distribution\n",
    "The pie chart illustrates the distribution of education levels of employees.\n",
    "\n",
    "With reference to the pie chart, there is a fairly balanced proportion across the three education categories. \n",
    "\n",
    "The nearly equal proportions indicate that the company has employees from an evenly distributed educational background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df['Education'].value_counts()\n",
    "y1 = x1.index\n",
    "plt.figure(figsize=(6, 6)) #size\n",
    "plt.pie(x1, labels=y1, autopct='%1.1f%%') #string format\n",
    "plt.title(\"Education Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de941a8",
   "metadata": {},
   "source": [
    "### City Distribution\n",
    "With most of the employees stemming from city C20, it could suggest 2 outcomes: \n",
    "\n",
    "1. The company could be based in city C20, where they tend to be more comfortable in hiring employees close to their location\n",
    "\n",
    "2. The company could be hiring employees from city C20 due to it being a metropolis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf57d99-7c27-449f-a3fb-07eb9410d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts = df['City'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.bar(city_counts.index, city_counts.values)\n",
    "\n",
    "plt.title(\"City Distribution\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Number of Employees\")\n",
    "plt.xticks(rotation=45) \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebc51a",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aa05b1",
   "metadata": {},
   "source": [
    "### Education and Salary\n",
    "With reference to the boxplot, it shows that the average salary between the various education level do not vary by a significant amount.\n",
    "\n",
    "However, it can be seen that with a higher level of education, there is a strong correlation to greater salary as seen by a larger number of outliers receiving a higher salary for those employees that has a Master degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42248e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "education_levels = df['Education'].unique()\n",
    "\n",
    "education_groups = [df[df['Education'] == level]['Salary'] for level in education_levels]\n",
    "\n",
    "plt.boxplot(education_groups, tick_labels=education_levels)\n",
    "\n",
    "plt.title('Bivariate Analysis: Education vs. Salary')\n",
    "plt.xlabel('Education Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0ec14",
   "metadata": {},
   "source": [
    "### Salary against Designation\n",
    "With reference to the boxplot, there is a clear positive relationship between the amount of salary received and their designation as seen by the increasing mean salary of employees at a certain designation. \n",
    "\n",
    "However, as seen from the spread of each boxplot, it is interesting to note that having a higher designation level does not necessarily mean that one would have a higher salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5bde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "designation_levels = df['Designation'].unique()\n",
    "\n",
    "designation_groups = [df[df['Designation'] == level]['Salary'] for level in designation_levels]\n",
    "\n",
    "plt.boxplot(designation_groups, tick_labels=designation_levels)\n",
    "\n",
    "plt.title('Bivariate Analysis: Salary vs Designation')\n",
    "plt.xlabel('Designation Level')\n",
    "plt.ylabel('Salary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d6bd5",
   "metadata": {},
   "source": [
    "### Salary against Gender\n",
    "The box plot provides a comparative analysis of the salary distribution by gender.\n",
    "\n",
    "With reference to the boxplot, both genders exhibit similar median salaries, indicating that, on average, salaries are fairly balanced. \n",
    "\n",
    "However, there is a significant spread in salaries, with some outliers extending well beyond the upper quartile for both categories, suggesting that a portion of employees earns substantially more than the typical salary range, probably indicating that they are holding higher designation levels in the company (as seen from the boxplot of Salary vs Designation).\n",
    "\n",
    "Furthermore, the presence of more extreme outliers among males may imply that a higher number of men occupy higher designation level or experience greater salary variation than women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "genders = df['Gender'].unique()\n",
    "\n",
    "gender_groups = [df[df['Gender'] == gender]['Salary'] for gender in genders]\n",
    "\n",
    "plt.boxplot(gender_groups, tick_labels=genders)\n",
    "\n",
    "plt.title('Bivariate Analysis: Salary vs Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Salary')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a61f39",
   "metadata": {},
   "source": [
    "### Salary against Age\n",
    "We wanted to confirm our hypothesis that an employee being older in terms on Age would incur a higher salary. We believed that Age would equate to more experience and thus command a higher salary. \n",
    "\n",
    "After creating the scatterplot, the regression line does show a slight linear relationship between Salary and Age, but the salary ranges seem evenly distributed across all age ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = df['Age']\n",
    "salary = df['Salary']\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.scatter(age,salary)\n",
    "plt.title(\"Plot of Salary vs Age\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Salary\")\n",
    "# drawing the regression line\n",
    "line = 1000.01 * age + 25672.63\n",
    "fig = plt.plot(age,line, lw=4, c='orange', label = 'Regression Line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36970c8",
   "metadata": {},
   "source": [
    "### Total_Sales_Acquired against Gender\n",
    "After looking at the difference between salary of both genders, we were curious regarding the sales difference between the genders. \n",
    "\n",
    "With reference to boxplot, the median total sales appear to be similar for both genders, suggesting that, on average, there is no significant difference in sales performance between men and women. \n",
    "\n",
    "However, both distributions show a considerable number of outliers. This might be because of the large spread of the distribution of the total sales acquired as seen above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f3c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Sales_Acquired'] = pd.to_numeric(df['Total_Sales_Acquired'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "genders = df['Gender'].unique()\n",
    "\n",
    "gender_groups = [df[df['Gender'] == gender]['Total_Sales_Acquired'] for gender in genders]\n",
    "\n",
    "plt.boxplot(gender_groups, tick_labels=genders)\n",
    "\n",
    "plt.title('Bivariate Analysis: Total Sales Acquired vs Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Total_Sales_Acquired')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c3d1fe",
   "metadata": {},
   "source": [
    "### Total_Sales_Acquired against Age\n",
    "We wanted to confirm our hypothesis that an employee being older in terms on Age would incur more Total Sales Acquired. We believed that Age would equate to more experience, and thus be able to perform better in terms of sales. \n",
    "\n",
    "After creating the scatterplot, the regression line does show a slight linear relationship between Total Sales Acquired and Age, but it seems that the age where the most Total Sales would occur is around the age of 40. This could be because as employees grow older after 40, they may be less focused on their performance due to other factors such as family or health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0febb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = df['Age']\n",
    "sales = df['Total_Sales_Acquired']\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.scatter(age,sales)\n",
    "plt.title(\"Plot of Total Sales Acquired vs Age\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Total Sales Acquired\")\n",
    "# drawing the regression line\n",
    "line2 = 400776.15 * age + -8905221.80\n",
    "fig = plt.plot(age,line2, lw=4, c='orange', label = 'Regression Line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad3551",
   "metadata": {},
   "source": [
    "### Total_Sales_Acquired against Salary\n",
    "We wanted to investigate the relationship beteeen Total Sales Acquired and Salary, and believed that they are linearly related as a better sales performance would justify an increase in salary. \n",
    "\n",
    "After running the data through a scatterplot, we observed that there is a linear relationship between Total Sales Acquired and Salary, but the relationship may not be as strong as we thought, as there is a significant portion of outliers where Total Sales seems to not affect the Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa168e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = df[\"Salary\"]\n",
    "sales = df['Total_Sales_Acquired']\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.scatter(salary,sales)\n",
    "plt.title(\"Plot of Total Sales Acquired vs Salary\")\n",
    "plt.xlabel(\"Salary\")\n",
    "plt.ylabel(\"Total Sales Acquired\")\n",
    "# drawing the regression line\n",
    "line3 = 122.09 * salary + -2657931.02\n",
    "fig = plt.plot(salary,line3, lw=4, c='orange', label = 'Regression Line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df78b38",
   "metadata": {},
   "source": [
    "### Total_Sales_Acquired against Quarterly_Rating\n",
    "We wanted to investigate the relationship between Total Sales Acquired and Quarterly Rating as we believed that having a higher rating will translate to have higher total sales.\n",
    "\n",
    "The bar chart illustrates a positive correlation between the Quarterly Rating and the Mean Total Sales Achieved. Employees with higher quarterly ratings tend to achieve greater total sales on average. This trend indicates that as ratings improve, sales also increase, demonstrating that performance ratings are a reliable reflection of each employee's sales productivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56146c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sales = df.groupby(\"Quarterly_Rating\")[\"Total_Sales_Acquired\"].mean()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(mean_sales.index, mean_sales.values)\n",
    "\n",
    "plt.xlabel(\"Quarterly Rating\", fontsize=12)\n",
    "plt.ylabel(\"Mean Total Sales Acquired\", fontsize=12)\n",
    "plt.title(\"Mean Total Sales Acquired vs. Quarterly Rating\", fontsize=14)\n",
    "plt.xticks(mean_sales.index)  # Ensure x-axis labels match the rating values\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939df9c",
   "metadata": {},
   "source": [
    "### Total_Sales_Acquired against Designation_Level\n",
    "We wanted to investigate the relationship between Total Sales Acquired and Designation as we believed that they are related as a higher the designation will reflect a greater total sales they made. \n",
    "\n",
    "The bar chart illustrates the relationship between total sales and employee designation levels. It shows a clear upward trend, indicating that as designation levels increase, total sales also rise. Employees in designation level 1 have the lowest total sales, while those in designation level 5 achieve the highest sales. This suggests that higher designations, which likely correlate with increased experience, responsibilities, and authority, contribute to improved sales performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(df['Designation'],df['Total_Sales_Acquired'])\n",
    "plt.title(\"Total Sales Acquired and Designation\")\n",
    "plt.xlabel(\"Designation\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01e7ee",
   "metadata": {},
   "source": [
    "## Part II: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b8ca9",
   "metadata": {},
   "source": [
    "## 1. Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86840cad",
   "metadata": {},
   "source": [
    "### 1.1 Regression Problem\n",
    "Salary plays a significant factor in a company's expenditure and employees are always concerned regarding how much salary they should receive. Hence, we want to have the ability to the predict and determine the salary of employees based on various attributes\n",
    "\n",
    "### 1.2 Classifiction Problem\n",
    "Employee designation level plays an important role in determining employee responsibilities, salary, and promotions, but companies may lack efficient ways to predict and determine employee's designation level based on various attributes.\n",
    "\n",
    "### 1.3 Chosen Problem\n",
    "We will choose to tackle the classification problem of determining employees' designation level. The goal of this project is to create a classification model using the demographic information provided by the dataset can forecast an employee's designation level.\n",
    "\n",
    "### 1.4 Dependent Variable\n",
    "Designation Level. It helps determine whether workers are in the correct existing designation levels based on their various attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d3925",
   "metadata": {},
   "source": [
    "### 2. Model training [30% of Part II]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71218b5",
   "metadata": {},
   "source": [
    "**a.** Perform feature selection. For each variable, decide if you want to include it as a feature and provide a justification. You may leverage on your analysis in Part I: EDA and/or perform additional analysis.\n",
    "\n",
    "**Response.** \n",
    "\n",
    "**b.** Split the dataset into train and test sets. Describe how you split step by step.\n",
    "\n",
    "**Response.** \n",
    "\n",
    "**c.** State the model(s) you will train, and explain your choice(s), in **no more than 50 words per model**. You only need to\n",
    "train one model, but if you do train more models, limit yourself to no more than three---Grading is based on the validity and soundness of your model, rather than the quantity.\n",
    "\n",
    "**Response.** \n",
    "\n",
    "**d.** For each model, perform the training, and report the trained parameters and the training scores, if applicable. \n",
    "\n",
    "**Response.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a6075",
   "metadata": {},
   "source": [
    "## 2. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39da0fd",
   "metadata": {},
   "source": [
    "We realised that some of our variables were considered numerical instead of categorical (join designation for example) due to them being a value (1, 2 etc). Thus we changed the values to strings to convert our data to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd10fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to categorical features\n",
    "df[\"Join_Designation\"] = df[\"Join_Designation\"].astype(str)\n",
    "df[\"Designation\"] = df[\"Designation\"].astype(str)\n",
    "df[\"Quarterly_Rating\"] = df[\"Quarterly_Rating\"].astype(str)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e36bd6",
   "metadata": {},
   "source": [
    "Next we will split our features to numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the into numerical and categorical\n",
    "datatypes = df.dtypes\n",
    "categorical_features = datatypes[datatypes==\"object\"].index\n",
    "numerical_features = datatypes[datatypes!=\"object\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0638a64f",
   "metadata": {},
   "source": [
    "### 2.1 Categorical Variables\n",
    "\n",
    "First, we investigate the distribution of each categorical variable by plotting barplots as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2512357",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(categorical_features)\n",
    "\n",
    "r, c = 3, 3\n",
    "fig, ax = plt.subplots(r, c, figsize=(40, 40))\n",
    "\n",
    "count = 1\n",
    "for i in range(n):\n",
    "    if (categorical_features[i] == \"Designation\"):\n",
    "        continue\n",
    "    feature = categorical_features[i]\n",
    "    ax = plt.subplot(r,c,count)\n",
    "    count += 1\n",
    "    ax.set_title(feature)\n",
    "    df[feature].value_counts(normalize=True).plot(kind='bar')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b6ad17",
   "metadata": {},
   "source": [
    "Based on the boxplots, we can see that the Join_Date and Last_Work_Date on their own do not bring much value to our analysis. We address this by finding the Number_Of_Working_Days by subtracting Join_Date from Last_Work_Date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Join_Date'] = pd.to_datetime(df['Join_Date'], format='%d/%m/%Y', errors='coerce')\n",
    "df['Last_Work_Date'] = pd.to_datetime(df['Last_Work_Date'], format='%d/%m/%Y', errors='coerce')\n",
    "df[\"Number_Of_Working_Days\"] = (df[\"Last_Work_Date\"] - df[\"Join_Date\"]).dt.days\n",
    "\n",
    "# Convert the estimated join date back to the original format\n",
    "df['Join_Date'] = df['Join_Date'].dt.strftime('%d/%m/%Y')\n",
    "df['Last_Work_Date'] = df['Last_Work_Date'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# adding the number of working days into the numerical list\n",
    "datatypes = df.dtypes\n",
    "numerical_features = datatypes[datatypes!=\"object\"].index\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))  # Correct way to create figure and axis\n",
    "ax.set_title(\"Number of Working Days\")\n",
    "df[\"Number_Of_Working_Days\"].value_counts(normalize=True).plot(kind='bar', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69ec9c",
   "metadata": {},
   "source": [
    "Furthermore, many of our variables have a dominant value. We decided not to select features where its mode has a frequency of more than 50%, as they are likely to have little explanatory power on the variation of `Salary`.\n",
    "\n",
    "We removed the Date feature as we found it to be irrelevant to the Salary to be determined. (Since the date is simply the date the data was measured, and has no effect on the variables themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a60941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 'important' categorical features i.e. those with mode having freq < 50%\n",
    "\n",
    "categorical_important = []\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if (feature == \"Date\" or feature == \"Join_Date\" or feature == \"Last_Work_Date\" or feature == \"Designation\"):\n",
    "        continue \n",
    "    highest = df[feature].value_counts(normalize=True).iloc[0]\n",
    "    if highest<0.5:\n",
    "        categorical_important.append(feature)\n",
    "print(f\"Variables with mode contributing <50% are : {categorical_important}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9793488",
   "metadata": {},
   "source": [
    "Based on the criteria we would keep these three categorical variables.  We next look into how `Deisgnation` is related to these variables.  For each categorical variable, we plot a stacked bar plot and see the distribution of `Designation` with respect to the categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(categorical_important)\n",
    "r, c = 2,2\n",
    "fig, ax = plt.subplots(r, c, figsize=(20,15))\n",
    "\n",
    "for i in range(n):\n",
    "    feature = categorical_important[::-1][i]\n",
    "    ax = plt.subplot(r,c,i+1)\n",
    "    pd.crosstab(df[feature], df[\"Designation\"], normalize=\"index\").plot.bar(stacked=True, ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bafcfcf",
   "metadata": {},
   "source": [
    "Based on a visual observation we noted the following:\n",
    "- For `Join_Designation`, those who have higher `Join_Designation` have a higher proportion of them having higher `Designation` than those with lower `Join_Designation`\n",
    "\n",
    "For the other variables, a visual inspection does not reveal any potential relationship to `Designation`\n",
    "\n",
    "Hence based on an EDA on the categorical variables, we will only keep `Join_Designation`. Furthermore, we would encode the categorical values with a numerical value. We could do that because `Join_Designation` is ordinal, meaning there is a natural order attached to the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding of join designation\n",
    "df[\"Join_Designation\"] = df[\"Join_Designation\"].astype(float)\n",
    "\n",
    "categorical_features_selected = [\"Join_Designation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b4c0a-02e6-4d31-a841-4368b4c117b6",
   "metadata": {},
   "source": [
    "### 2.2 Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0e535-315b-4e0e-8096-111e5d2e0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_features].hist(layout=(9,5), figsize=(20,30))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f234cba-e12e-4f5d-a4fe-5625301c0813",
   "metadata": {},
   "source": [
    "As per visual observation,\n",
    "- Emp_ID is an index hence it would not exploratory power\n",
    "- Age has a good variation.\n",
    "- Salary has a good variation.\n",
    "- Total_Sales_Acquired and Number_Of_Working_Days have slight variations, which implies their inability to appropriately predict Designation.\n",
    "\n",
    "To further determine if this would be correct, we must conduct another visual observation by comparing the IQR with the median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d3bf7-0f8b-41b5-9f11-61eb0178ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected = [\"Age\", \"Total_Sales_Acquired\", \"Salary\", \"Number_Of_Working_Days\"]\n",
    "\n",
    "n = len(features_selected)\n",
    "\n",
    "r, c = 1,4\n",
    "fig, ax = plt.subplots(r, c, figsize=(20,4))\n",
    "\n",
    "for i in range(n):\n",
    "    feature = features_selected[i]\n",
    "    ax = plt.subplot(r,c,i+1)\n",
    "    ax.set_title(feature)\n",
    "    df[['Designation',feature]].boxplot(by='Designation', ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36886be-50d2-456c-b10a-be8e9d67745c",
   "metadata": {},
   "source": [
    "We have chosen Age and Salary, given that the IQR adjusts appropriately. The median values increases with the IQR, which implies that the features are correlated to Designation.\n",
    "\n",
    "Dependent Variable: Designation.\n",
    "\n",
    "Independent Variable/s: Age and Salary.\n",
    "\n",
    "Instinctively, we can conclude that the selected features are not correlated with one another. However, to further prove this, we have attached a heatmap that would indicate such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8911c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected = [\"Age\", \"Salary\"]\n",
    "print(f\"Updated Selected Features: {features_selected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8dc8f-f451-466d-ab25-5adf51c527ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = df[features_selected].corr()**2\n",
    "sns.heatmap(r2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ba174-6c0b-46f5-a40a-a8c337b2fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature that has the most number of features\n",
    "# it is correlated with, beyond certain threshold\n",
    "\n",
    "def most_corr_feature(data, threshold):\n",
    "    r2_matrix = abs(data.corr())\n",
    "    count = r2_matrix[r2_matrix>threshold].count()\n",
    "    return count.sort_values(ascending=False).index[0]\n",
    "\n",
    "# return true if all the features are uncorrelated,\n",
    "# as defined by a threshold\n",
    "\n",
    "def all_features_uncorr(data, threshold):\n",
    "    r2_matrix = abs(data.corr())\n",
    "    n = len(r2_matrix)\n",
    "    return r2_matrix[r2_matrix>threshold].count().sum()==n\n",
    "\n",
    "# get a set of uncorrelated features\n",
    "\n",
    "def get_uncorr_features(data, threshold):\n",
    "    features = data.columns.tolist()\n",
    "    while all_features_uncorr(data[features], threshold) == False:\n",
    "        most_corr_fea = most_corr_feature(data[features], threshold)\n",
    "        features.remove(most_corr_fea)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e869060-cc8d-4607-8909-35a62137f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features_selected = get_uncorr_features(df[features_selected], 0.5)\n",
    "print(f\"Final Selected Numerical Features : {numerical_features_selected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495f82b-fdfc-402e-8178-8db7efb47646",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(numerical_features_selected)\n",
    "\n",
    "r, c = 1,2\n",
    "fig, ax = plt.subplots(r, c, figsize=(10,5))\n",
    "\n",
    "for i in range(n):\n",
    "    feature = features_selected[i]\n",
    "    ax = plt.subplot(r,c,i+1)\n",
    "    ax.set_title(feature)\n",
    "    df[['Designation',feature]].boxplot(by='Designation', ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1b65d-0cda-44fb-999f-6656e9886652",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected = categorical_features_selected + numerical_features_selected\n",
    "print(f\"Selected features : {features_selected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9383e0f6-e762-4985-9f21-81a61f2875d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_inc_dependent = features_selected + ['Designation']\n",
    "df[features_inc_dependent].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dbe543",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53beacf5",
   "metadata": {},
   "source": [
    "**a.** For each model, predict the response variable on the test set.\n",
    "\n",
    "**Response.** \n",
    "\n",
    "**b.** Describe the metric you use to evaluate your model(s). Report the test scores for each model.\n",
    "\n",
    "The data is separated into training and testing sets using the *train_test_split* function. The function divides it into two sections at random: one for testing (X_test, y_test) and one for training (X_train, y_train), where y is the data we're attempting to predict and X is used to predict target variables. After that, we divide the data into two parts: 75% will be used for training and 25% will be used for testing. \n",
    "\n",
    "\n",
    "**c.** If you trained more than one model, identify the final model you would choose for the prediction task, and explain your choice, **in no more than 50 words**.\n",
    "\n",
    "We ended up using 3 models which are Gaussian Naive Bayes, Logistic Regression and k-Nearest Neighbor.\n",
    "\n",
    "**Gaussian Naive Bayes**\n",
    "\n",
    "Since this model assumes feature independence and performs well with tiny datasets, we decided to employ it. It provides a solid foundation for classification activities and is quick and easy to understand.\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "Since this model is a popular linear model for classification, we decided to utilize it. It efficiently manages numerical and categorical features and offers probability. \n",
    "\n",
    "**k-Nearest Neighbor**\n",
    "\n",
    "We selected this model because to its ability to efficiently handle mixtures of continuous and categorical numerical data. Additionally, KNN classifies based on similarity, where our feature variables may have a close relationship with our predicted designation values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40073510",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[features_selected]\n",
    "y = df[[\"Designation\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e94a8-23a5-4d4b-aff2-a16c9abd8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding and scaling\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Convert y to 1D\n",
    "y_train = y_train.values.ravel()\n",
    "\n",
    "# Encoding categorical features - one hot encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_features_selected)\n",
    "    ], \n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Apply transformation before scaling\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "# Apply scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# Initialize KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Use GridSearchCV to find the optimal k using cross-validation\n",
    "grid_search = GridSearchCV(knn, {'n_neighbors': range(1, 16)}, cv=5)  # try k = 1 to k = 15\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "print(best_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f99db-0870-4c9a-b305-b5ff2f5de353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model1 = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "model2 = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "model3 = grid_search.best_estimator_\n",
    "model3.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67723601-6ea4-408d-8d89-9fd3c82f316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 1 evaluation\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# predict based on test set\n",
    "\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# compare with ground truth\n",
    "\n",
    "acc =  accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Model 1 Evaluation\")\n",
    "print(\"-----------------\")\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f9e25-94a8-4586-8070-5f925db3dde4",
   "metadata": {},
   "source": [
    "For Gaussian Naive Bayes, our trained parameters are the Class Prior Probabilities, Means of features for each class and Variances of Features for each class. For this model, we also have the following training scores: \n",
    "- Accuracy -> 0.685\n",
    "- F1-score -> 0.681\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd086745-6f92-453a-a92f-3d9cf4e80b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 2 evaluation\n",
    "\n",
    "# predict based on test set\n",
    "\n",
    "y_pred = model2.predict(X_test)\n",
    "\n",
    "# compare with ground truth\n",
    "\n",
    "acc =  accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Model 2 Evaluation\")\n",
    "print(\"-----------------\")\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71dcc7-0451-4892-b82c-b7a033fcdc95",
   "metadata": {},
   "source": [
    "For Logistic Regression, our trained parameters are the Coefficients and Intercept. For this model, we also have the following training scores:\n",
    "- Accuracy -> 0.503\n",
    "- F1-score -> 0.499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d42dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 3 evaluation\n",
    "\n",
    "# predict based on test set\n",
    "y_pred = model3.predict(X_test_scaled)\n",
    "\n",
    "# compare with ground truth\n",
    "\n",
    "acc =  accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Model 3 Evaluation\")\n",
    "print(\"-----------------\")\n",
    "print(f\"k-value: {best_k}\")\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea68c2-caa4-4ebd-8df7-4aad6ea4f482",
   "metadata": {},
   "source": [
    "For k-Nearest Neighbor, our trained parameters is the k-value, where we found the best k-value using grid search. \n",
    "- Optimal k -> 14\n",
    "  \n",
    "For this model, we also have the following training scores:\n",
    "- Accuracy -> 0.850\n",
    "- F1-score -> 0.847"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ed307",
   "metadata": {},
   "source": [
    "## 4. Findings and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe647645",
   "metadata": {},
   "source": [
    "### 4.1 Interpretation of Model and Insights\n",
    "Our final model implies that an increase in `Join_Designation`, `Salary` or `Age` would result in a higher `Designation`. This is consistent with intuition, as a higher `Age` implies more experience, and higher `Salary` and `Join_Designation` imply they are good in their work. This would logically result in a higher `Designation`.\n",
    "\n",
    "### 4.2 Lessons Learnt From Project\n",
    "We learned how to filter the variables that are important to our analysis, as not every variable is important. We also learned about the importance of statistical analysis in the real world, in the case of our project, to visualise and predict a very real variable people would want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee09ec",
   "metadata": {},
   "source": [
    "## 5. Non-technical protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e97393",
   "metadata": {},
   "source": [
    "**a.** Describe the detailed contribution of each team member, including both the tangible (e.g., implementation, testing, writing) and intangible (e.g., generating ideas, planning, leadership) efforts.\n",
    "\n",
    "**Response.** \n",
    "\n",
    "**b.** List any references and sources you have cited.\n",
    "\n",
    "GridSearchCV documentation - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "KNeighborClassifier documentation - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
